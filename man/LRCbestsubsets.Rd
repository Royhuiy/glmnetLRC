% Generated by roxygen2 (4.0.1): do not edit by hand
\name{LRCbestsubsets}
\alias{LRCbestsubsets}
\alias{coef.LRCbestsubsets}
\alias{plot.LRCbestsubsets}
\alias{print.LRCbestsubsets}
\alias{summary.LRCbestsubsets}
\title{Best subsets logistic regression classifier (LRC) with arbitrary loss function}
\usage{
LRCbestsubsets(truthLabels, predictors, lossMat, weight = rep(1,
  NROW(predictors)), tauVec = seq(0.1, 0.9, by = 0.05), naFilter = 0.6,
  cvFolds = 5, cvReps = 100, masterSeed = 1, cores = 1,
  cluster = NULL, verbose = FALSE, ...)

\method{print}{LRCbestsubsets}(LRCbestsubsets_object)

\method{summary}{LRCbestsubsets}(LRCbestsubsets_object)

\method{plot}{LRCbestsubsets}(LRCbestsubsets_object, ...)

\method{coef}{LRCbestsubsets}(LRCbestsubsets_object)
}
\arguments{
\item{truthLabels}{A factor with two levels containing the true labels for each
observation.
If it is more desirable to correctly predict one of the two classes over the other,
the second level of this factor should be the class you are most interested in
predicting correctly.}

\item{predictors}{A matrix whose columns are the explanatory regression variables}

\item{lossMat}{A loss matrix of class \code{lossMat}, produced by
\code{\link{lossMatrix}}, that specifies the penalties for classification errors.}

\item{weight}{The observation weights that are passed to \code{\link{glm}}.
The default value is 1 for each observation. Refer to the \code{weights} arguments of
\code{\link{glm}} for further information.}

\item{tauVec}{A sequence of tau threshold values for the
logistic regression classifier.}

\item{naFilter}{The maximum proportion of data observations that can be missing
for a given predictor
(a column in \code{predictors}). If, for a given predictor, the proportion of
sample observations which are NA is greater than \code{naFilter}, the predictor is
not included in the elastic net model fitting.}

\item{cvFolds}{The number of cross validation folds. \code{cvFolds = NROW(predictors)}
gives leave-one-out cross validation.}

\item{cvReps}{The number of cross validation replicates, i.e., the number
of times to repeat the cross validation
by randomly repartitioning the data into folds.}

\item{masterSeed}{The random seed used to generate unique seeds for
each cross validation replicate.}

\item{cores}{The number of cores on the local host
to use in parallelizing the training.  Parallelization
takes place at the \code{cvReps} level, i.e., if \code{cvReps = 1}, parallelizing would
do no good, whereas if \code{cvReps = 2}, each rep would be run separately in its
own thread if \code{cores = 2}.
Parallelization is executed using \code{\link{parLapply}} from the
\pkg{parallel} package.}

\item{cluster}{An object that inherits from class \code{cluster} that is returned by
\code{\link{makeCluster}} in package \pkg{parallel}. The \code{cores} argument is
ignored when a \code{cluster} is provided.}

\item{verbose}{A logical, set to \code{TRUE} to receive messages regarding
the progress of the training algorithm.}

\item{\dots}{Additional named arguments to \code{\link{bestglm}} and \code{\link{glm}}.}

\item{LRCbestsubsets_object}{An object of class \code{LRCbestsubsets},
returned by \code{LRCbestsubsets},
which contains the best subsets logistic regression model and the optimal threshold,
\eqn{\tau}.}

\item{\dots}{Additional arguments to default S3 method \code{\link{symbols}}.}
}
\value{
Returns an object of class \code{LRCbestsubsets}, which
inherits from classes \code{glm} and \code{lm}.  It contains the
object returned by \code{\link{glm}} that has been fit to all the data using
the the best predictors identified by \code{\link{bestglm}}. It also contains the values
of the optimal estimates of \eqn{\tau} from the individual cross validation replicates,
along with the median of those estimates, which constitutes the overall estimate of
the optimal \eqn{\tau}.

Methods \code{print}, \code{summary}, \code{plot}, and \code{coef} are provided.
In addition, many of the S3 methods for \code{glm} and \code{lm} can be applied to
the object returned by \code{LRCbestsubets}.  See \code{methods(class = "glm")} or
\code{methods(class = "lm")} to see a listing of these methods.
}
\description{
This functions uses the \code{\link{bestglm}}
function from the \pkg{bestglm}
package to fit a best subsets logistic regression model and then estimate an optimal
binary classification threshold using cross validation, where optimality is defined
by minimizing an arbitrary, user-specified discrete loss function.
}
\details{
For a given partition of the training data, cross validation is
performed to estimate the optimal values of
\eqn{\tau}
which is used to dichotomize the probability predictions of the
logistic regression model into binary outcomes.
(Specifically, if the probability an observation
belongs to the second level of \code{truthLabels} exceeds \eqn{\tau}, it is
classified as belonging to that second level).  In this case, optimality is defined
as the the value of \eqn{\tau} that minimize the loss function defined by
\code{lossMat}.

\code{LRCbestsubsets} searches for the optimal values of \eqn{\tau} by
fitting the best subsets logistic regression model and then calculating
the expected loss for each \eqn{\tau} in \code{tauVec}, and the
value of \eqn{\tau} that results in the lowest risk designates the
optimal threshold for a given cross-validation partition of the data.

This process is repeated \code{cvReps} times, where each time a different random
partition of the data is created using its own seed, resulting in another
'optimal' estimate of \eqn{\tau}.  The final estimate of
\eqn{\tau} is the median of those estimates.

The final best subsets logistic regression classfier is given by estimating
the best subsets regression
coefficients using all the training data and then using the optimal \eqn{\tau} to
dichotomize the probability predictions from the logistic regression model into
one of two binary categories.
}
\section{Methods (by generic)}{
\itemize{
\item \code{print}: Displays the overall optimized value of
\eqn{\tau} and prints (using \code{print.glm}) the best logistic regression model.
Invisibly returns the threshold estimate for each cross validation replicate as a
data frame.

\item \code{summary}: Displays the overall optimized value of
\eqn{\tau} and the summary (using \code{summary.glm}) of the best logistic regression
model.

\item \code{plot}: Produces a scatter plot of \eqn{\tau} vs. the expected
loss for each of the monte-carlo replicates of the
cross-validation partitioning.  This can provide a sense of how stable the estimates
are across the cross-validation replicates. Also plots the \code{glm} object using
\code{plot.glm}

\item \code{coef}: Calls the \code{\link{coef.glm}} method
on the best subsets regression and returns the logistic regression coefficients
}}
\examples{
# Load the Mojave data
data(Mojave)
str(Mojave)

# Here we select the predictor variables (remove the location variables and the response)
predictors <- Mojave[,-c(1,2,11)]

# Create a vector for the response (presence/absence of cheat grass)
cheat <- Mojave$cheatGrass

# "0" is no cheatgrass observed and "1" is cheatgrass.  Note how "1" is the second level
# of the factor. This second level is the level for which the summary methods calculate
# the sensitivity. So the factor coding here is perfect, since we are most interested
# in predicting the presence of cheatgrass with greatest sensitivity (power).
levels(cheat)

# Specify the loss matrix. In this example, we specify the penalty for missing
# cheatgrass as 2, while the penalty for predicting it falsely is 1.
lM <- lossMatrix(c("0","0","1","1"),
                 c("0","1","0","1"),
                 c(0,   1,  2,  0))
print(lM)

# Train the best subsets logistis regression classifier (in particular, identify
# the optimal threshold, tau, that minimizes the loss).  As this takes some time,
# we'll skip this step
\dontrun{LRCbestsubsets_fit <- LRCbestsubsets(cheat, predictors, lM,
                                              cvReps = 100, cvFolds = 5,
                                              cores = max(1, detectCores() - 1))}


# We'll load the precalculated model fit instead
\donttest{data(LRCbestsubsets_fit)}

\dontshow{
# Here is a call to LRCbestsubsets() that will run quickly for testing purposes
LRCbestsubsets_fit <- LRCglmnet((cheat, predictors, lM, cvReps = 3,
                                 cvFolds = 3, cores = max(1, detectCores() - 1),
                                 tauVec = c(0.4, 0.5)))
}


# Demonstrate the various methods for LRCbestsubsets() output
# (print, summary, plot, coef)
print(LRCbestsubsets_fit)

# Assigning the output of print extracts the set of optimal taus that
# were identified during the cross validation replicates
o <- print(LRCbestsubsets_fit)
o

# Sumarize the best fit LRC
summary(LRCbestsubsets_fit)

# Plot the loss as a function of the optimal taus for each cross validation
# replicate.  Also plot the bestsubsets logistic regression glm object.
plot(LRCbestsubsets_fit)

# Display the coefficients
coef(LRCbestsubsets_fit)

# Make predictions using final model on the training data
out <- predict(LRCbestsubsets_fit, Mojave, truthCol = "cheatGrass")

head(out)

# Calculate the performance of predictions
summary(out)
}
\author{
Landon Sego
}

