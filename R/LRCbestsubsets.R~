##' Best subsets logistic regression classifier (LRC) with arbitrary loss function
##'
##' @description
##' This functions uses the \code{\link{bestglm}}
##' function from the \pkg{bestglm}
##' package to fit a best subsets logistic regression model and then estimate an optimal
##' binary classification threshold using cross validation, where optimality is defined
##' by minimizing an arbitrary, user-specified discrete loss function.
##'
##' @details
##' For a given partition of the training data, cross validation is
##' performed to estimate the optimal values of
##' \eqn{\tau}
##' which is used to dichotomize the probability predictions of the
##' logistic regression model into binary outcomes.
##' (Specifically, if the probability an observation
##' belongs to the second level of \code{truthLabels} exceeds \eqn{\tau}, it is
##' classified as belonging to that second level).  In this case, optimality is defined
##' as the the value of \eqn{\tau} that minimize the loss function defined by
##' \code{lossMat}.
##'
##' \code{LRCbestsubsets} searches for the optimal values of \eqn{\tau} by
##' fitting the best subsets logistic regression model and then calculating
##' the expected loss for each \eqn{\tau} in \code{tauVec}, and the
##' value of \eqn{\tau} that results in the lowest risk designates the
##' optimal threshold for a given cross-validation partition of the data.
##'
##' This process is repeated \code{cvReps} times, where each time a different random
##' partition of the data is created using its own seed, resulting in another
##' 'optimal' estimate of \eqn{\tau}.  The final estimate of
##' \eqn{\tau} is the median of those estimates.
##'
##' The final best subsets logistic regression classfier is given by estimating
##' the best subsets regression
##' coefficients using all the training data and then using the optimal \eqn{\tau} to
##' dichotomize the probability predictions from the logistic regression model into
##' one of two binary categories.
##'
##' @author Landon Sego
##'
##' @rdname LRCbestsubsets
##'
##' @export
##'
##' @param truthLabels A factor with two levels containing the true labels for each
##' observation.
##' If it is more desirable to correctly predict one of the two classes over the other,
##' the second level of this factor should be the class you are most interested in
##' predicting correctly.
##'
##' @param predictors A matrix whose columns are the explanatory regression variables
##'
##' @param lossMat A loss matrix of class \code{lossMat}, produced by
##' \code{\link{lossMatrix}}, that specifies the penalties for classification errors.
##'
##' @param weight The observation weights that are passed to \code{\link{glm}}.
##' The default value is 1 for each observation. Refer to the \code{weights} arguments of
##' \code{\link{glm}} for further information.
##'
##' @param tauVec A sequence of tau threshold values for the
##' logistic regression classifier.
##'
##' @param naFilter The maximum proportion of data observations that can be missing
##' for a given predictor
##' (a column in \code{predictors}). If, for a given predictor, the proportion of
##' sample observations which are NA is greater than \code{naFilter}, the predictor is
##' not included in the elastic net model fitting.
##'
##' @param cvFolds The number of cross validation folds. \code{cvFolds = NROW(predictors)}
##' gives leave-one-out cross validation.
##'
##' @param cvReps The number of cross validation replicates, i.e., the number
##' of times to repeat the cross validation
##' by randomly repartitioning the data into folds.
##'
##' @param masterSeed The random seed used to generate unique seeds for
##' each cross validation replicate.
##'
##' @param cores The number of cores on the local host
##' to use in parallelizing the training.  Parallelization
##' takes place at the \code{cvReps} level, i.e., if \code{cvReps = 1}, parallelizing would
##' do no good, whereas if \code{cvReps = 2}, each rep would be run separately in its
##' own thread if \code{cores = 2}.
##' Parallelization is executed using \code{\link{parLapply}} from the
##' \pkg{parallel} package.
##'
##' @param cluster An object that inherits from class \code{cluster} that is returned by
##' \code{\link{makeCluster}} in package \pkg{parallel}. The \code{cores} argument is
##' ignored when a \code{cluster} is provided.
##'
##' @param verbose A logical, set to \code{TRUE} to receive messages regarding
##' the progress of the training algorithm.
##'
##' @param \dots Additional named arguments to \code{\link{bestglm}} and \code{\link{glm}}.
##'
##' @return
##' Returns an object of class \code{LRCbestsubsets}, which
##' inherits from classes \code{glm} and \code{lm}.  It contains the
##' object returned by \code{\link{glm}} that has been fit to all the data using
##' the the best predictors identified by \code{\link{bestglm}}. It also contains the values
##' of the optimal estimates of \eqn{\tau} from the individual cross validation replicates,
##' along with the median of those estimates, which constitutes the overall estimate of
##' the optimal \eqn{\tau}.
##'
## @references
##'
## @examples

## TODO:  Test with factor predictors

LRCbestsubsets <- function(truthLabels, predictors, lossMat,
                           weight = rep(1, NROW(predictors)),
                           tauVec = seq(0.1, 0.9, by = 0.05),
                           naFilter = 0.6,
                           cvFolds = 5,
                           cvReps = 100,
                           masterSeed = 1,
                           cores = 1,
                           cluster = NULL,
                           verbose = FALSE,
                           ...) {

  
  
  # Checks on inputs
  stopifnot(NCOL(predictors) > 1,
#            is.matrix(predictors), I don't think a matrix is necessary for GLM
#            is.numeric(predictors),
            length(unique(truthLabels)) == 2,
            length(truthLabels) == NROW(predictors),
            inherits(lossMat, "lossMat"),
            is.numeric(tauVec),
            all(tauVec < 1) & all(tauVec > 0),
            length(naFilter) == 1,
            is.numeric(naFilter),
            naFilter < 1 & naFilter > 0,
            length(cvFolds) == 1,
            length(cvReps) == 1,
            is.numeric(cvFolds),
            is.numeric(cvReps),
            cvFolds %% 1 == 0,
            cvFolds >= 2,
            cvFolds <= NROW(predictors),
            cvReps %% 1 == 0,
            cvReps > 0,
            is.numeric(masterSeed))

  # Force the evaluation of the weight object immediately--this is IMPORTANT
  # because of R's lazy evaluation
  force(weight)

  ################################################################################
  # Data preparation
  ################################################################################

  # Get the data and filter missing values as neccessary
  d <- glmnetLRC:::dataPrep(truthLabels, predictors, weight, naFilter, verbose)

  # number of observations after filtering for NAs
  n <- length(d$truthLabels)

  # Report the number of observations
  if (verbose)
    cat(n, "observations are available for fitting the LRCbestsubsets model\n")

  ################################################################################
  # Set up cluster
  ################################################################################

  if (!is.null(cluster)) {
    if (!inherits(cluster, "cluster"))
      stop("'cluster' must inhererit the 'cluster' class")
  }


  # If the cluster is to be local, simply give the number of cores. If the
  # cluster is distributed, then a vector of nodeNames and a corresponding
  # vector denoting the number of cpus to use on each node must be supplied
  if (is.null(cluster)) {

   nCores <- detectCores()

    if (nCores < cores) {

      warning("Number of requested cores exceeds the number available on the host (",
              nCores, ")\n")

    }

    if (cvReps < cores) {

      warning("Number of cross validation replicates is less than the number of\n",
              "requested cores.  Setting number of cores to ", cvReps)

      cores <- cvReps

    }

    #cl <- makeCluster(cores)

  } else {

    # Cluster has been provided
    cl <- cluster

  }

  ################################################################################
  # Train via cross validation
  ################################################################################

  # Create the vector of seeds that will be used in the parallel call
  set.seed(masterSeed)
  seedVec <- unique(as.integer(runif(cvReps * 2, min = 1, max = cvReps * 10)))

  # Make sure the length of seedVec is >= cvReps.  If not, add more seeds
  i <- 0

  while ((length(seedVec) < cvReps) & (i < 20)) {

    seedVec <- unique(c(seedVec,
                        as.integer(runif((cvReps - length(seedVec)) * 10,
                                         min = 1, max = cvReps * 10))))

    i <- i + 1

  }

  # Randomly select a vector of seeds from the unique set
  seedVec <- sample(seedVec, cvReps)

  # Load the glmnetLRC package on the worker nodes
#  clusterEvalQ(cl, require(glmnetLRC))

  # Export the required objects to the##  worker nodes
##   clusterExport(cl, c("d",
##                       "tauVec",
##                       "n",
##                       "cvFolds",
##                       "lossMat",
##                       "verbose"),
##                 envir = environment())

  # Creating separate truthLabels and predictor objects
  truthLabels <- d$truthLabels
  predictors <- d$predictors
  
  # A wrapper function for calling single_LRCbestsubsets via parLapply
  trainWrapper <- function(seed) {

    single_LRCbestsubsets(truthLabels,
                          predictors,
                          lossMat,
                          d$weight,
                          tauVec,
                          cvFolds,
                          seed,
                          n,
                          verbose,
                          ...)

  } # trainWrapper

  # Excecute the training in parallel
#  parmEstimates <- list2df(parLapply(cl, seedVec, trainWrapper), row.names = 1:cvReps)
#  parmEstimates <- parLapply(cl, seedVec, trainWrapper)
  parmEstimates <- lapply(seedVec, trainWrapper)

  browser()


  
  # Now stop the cluster
  stopCluster(cl)

  # The final parameter estimates will be the (alpha, lambda, tau) centroid in
  # the parameter estimates

  # Use the median instead...
  finalParmEstimates <- apply(parmEstimates[,c("alpha", "lambda", "tau")], 2, median)

  ################################################################################
  # Create the final model from the averaged parameters
  ################################################################################

  # Create an aggregated lambda sequence to fit the final lasso logisitc
  # regression model. Per the documentation in glmnetfit, apparently it does better
  # with a sequence of lambdas during the fitting.  But the predict method only
  # operates on the optimal lambda
  sdLambda <- sd(parmEstimates$lambda)
  lambdaVec <- sort(c(finalParmEstimates["lambda"],
                      seq(finalParmEstimates["lambda"] + sdLambda,
                          max(finalParmEstimates["lambda"] - sdLambda, 1e-04),
                          length = 50)),
                      decreasing = TRUE)

  # Fit the model using the aggregate parameters
  glmnetFinal <- glmnet(d$predictors, d$truthLabels, weights = d$weight,
                        family = "binomial", lambda = lambdaVec,
                        alpha = finalParmEstimates["alpha"])

  # Return the optimal parameters to make graphical output
  glmnetFinal$parms <- parmEstimates

  # Return the aggregated optimal parameters (gridVals)
  glmnetFinal$optimalParms <- finalParmEstimates

  # Assign the class
  class(glmnetFinal) <- c("LRCbestsubsets", class(glmnetFinal))


  return(glmnetFinal)

} # LRCbestsubsets

##' @method print LRCbestsubsets
##'
##' @describeIn LRCbestsubsets Displays the overall optimized values of
##' \eqn{(\alpha, \lambda, \tau)}, with the corresponding degrees of freedom and
##' deviance.  Invisibly returns the same information as a matrix.
##'
##' @param LRCbestsubsets_object An object of class \code{LRCbestsubsets}, returned by \code{LRCbestsubsets},
##' which contains the optimally-trained elastic net logistic regression classifier
##'
##' @export

print.LRCbestsubsets <- function(LRCbestsubsets_object) {

  # Find the index of the optimal lambda in the glmnet object
  indexMatch <- order(abs(LRCbestsubsets_object$lambda -
                          LRCbestsubsets_object$optimalParms["lambda"]))[1]

  # Assemble the optimal parameters, including the df and the % deviance
  op <- matrix(c(LRCbestsubsets_object$df[indexMatch],
                 LRCbestsubsets_object$dev.ratio[indexMatch],
                 LRCbestsubsets_object$optimalParms),
               nrow = 1,
               dimnames = list(NULL, c("Df", "%Dev",
                                       names(LRCbestsubsets_object$optimalParms))))

  # Print the optimal parms
  cat("The optimal parameter values for the elastic net logistic regression fit: \n")
  print(op)

  # Invisibly return the optimal parms matrix
  invisible(op)

} # print.LRCbestsubsets

##' @method plot LRCbestsubsets
##'
##' @describeIn LRCbestsubsets Produces a pairs plot of the
##' \eqn{(\alpha, \lambda, \tau)} triples and their univariate histograms that
##' were identified as optimal for each of the monte-carlo replicates of the
##' cross-validation partitioning.  This can provide a sense of how stable the estimates
##' are across the cross-validation replicates.
##'
##' @param \dots Additional arguments to default S3 method \code{\link{pairs}}.
##' @export

plot.LRCbestsubsets <- function(LRCbestsubsets_object, ...){

  ## put histograms on the diagonal
  panel.hist <- function(x, ...) {

      usr <- par("usr"); on.exit(par(usr))
      par(usr = c(usr[1:2], 0, 1.5) )
      h <- hist(x, plot = FALSE)
      breaks <- h$breaks; nB <- length(breaks)
      y <- h$counts; y <- y/max(y)
      rect(breaks[-nB], 0, breaks[-1], y, col = "cyan", ...)
  }


  # Default parameter values
  defaultParms <- list(x = LRCbestsubsets_object$parms[,c("alpha", "lambda", "tau")],
                       labels = c(expression(alpha), expression(lambda), expression(tau)),
                       cex = 1.5,
                       pch = 1,
                       cex.labels = 2,
                       font.labels = 2,
                       bg = "light blue",
                       diag.panel = panel.hist,
                       main = paste("Optimal LRCbestsubsets parameters for",
                                    NROW(LRCbestsubsets_object$parms),
                                    "cross validation replicates"))


  # Create the parms list
  parmsList <- list(...)

  # Add in default parms not already present in parmslist
  parmsList <- c(parmsList, defaultParms[setdiff(names(defaultParms), names(parmsList))])

  # Make the pairs plot
  do.call(pairs, parmsList)

  invisible(NULL)

} # plot.LRCbestsubsets

##' @method coef LRCbestsubsets
##'
##' @describeIn LRCbestsubsets Calls the \code{\link{coef.glmnet}} method in \pkg{glmnet}
##' on the fitted glmnet object and returns the logistic regression coefficients
##' using the optimal values of \eqn{\alpha} and \eqn{\lambda}.
##'
##' @export

coef.LRCbestsubsets <- function(LRCbestsubsets_object) {

  glmnet:::coef.glmnet(LRCbestsubsets_object, s = LRCbestsubsets_object$optimalParms["lambda"])

} # coef.LRCbestsubsets

