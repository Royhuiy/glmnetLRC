## Train best subsets logistic regression classifier for a single cross validation
## run.  A helper function for LRCbestsubsets()

single_LRCbestsubsets <- function(truthLabels,
                                  predictors,
                                  lossMat,
                                  weight,
                                  tauVec,
                                  cvFolds,
                                  seed,
                                  n,
                                  verbose,
                                  ...){

  # ... are arguments passed to best glm and glm (other than 'family') should check this

  # Get the name of the truthLabels
  truthLabelName <- deparse(substitute(truthLabels))

  # Create a combined data matrix
  dataMatrix <- cbind(predictors, truthLabels)

  # Create column names for the matrix or data frame
  colnames(dataMatrix) <- c(colnames(predictors), truthLabelName)

  # Function to train and test over a single CV fold
  trainTest <- function(testSet) {

    # testset:  indexes in 1:n of the data set that will be used to test

    # Find the complement of the testSet
    trainSet <- sort(setdiff(1:n, testSet))

    # Geta subset of the data
    dataMatrixTrain <- dataMatrix[trainSet,]

    # Train the best subsets logistic regression
    bestSub <- bestglm(dataMatrixTrain, IC = "BIC", weights = weight[trainSet],
                       family = binomial, ...)

    # Because bestglm doesn't have a predict method (how stupid is that?), we'll
    # extract the predictors and fit the model using glm
    bestPred <- rownames(print(bestSub))[-1]

    # Select the terms of the best subsets
    bestDataMatrix <- dataMatrixTrain[, c(bestPred, truthLabelName)]

    # Now fit the best logistic regression model
    LRfit <- glm(as.formula(paste(truthLabelName, "~ .")),
                 data = bestDataMatrix, weight = weight[trainSet],
                 family = "binomial", ...)

    # Now test it
    out <- predLoss_LRCbestsubsets(LRfit, predictors[testSet,], truthLabels[testSet],
                                   lossMat, tauVec = tauVec, weight = weight[testSet])

    browser()

    return(out)

  } # trainTest


  # Generate the test folds
  testFolds <- parseJob(n, cvFolds, random.seed = seed)

  # Test/train over over the folds
  completeTest <- list2df(lapply(testFolds, trainTest))


  return(NULL)

  # Now summarize the loss over the cv folds, with a loss value for each
  # alpha, lambda, and tau combination for a given seed
  dfData <- list2df(dlply(completeTest,

                          .variables = c('alpha', 'lambda','tau'),

                          .fun = function(x){

                             # x = K x K data.frame of values for the K folds with
                             # same (alpha, lambda, tau, seed) parameter values.
                             Eloss <- sum(x$weightedSumLoss) / sum(x$sumWeights)

                             return(list('ExpectedLoss' = Eloss,
                                         'alpha' = unique(x$alpha),
                                         'tau' = unique(x$tau),
                                         'lambda' = unique(x$lambda)))
                           }),

                     row.names = NULL)


  if (any(is.na(dfData)))
    warning("Unexpected NA values are present in the cross\n",
             "validation results for replicate seed = ", seed, "\n")


  # Searching for the minimum by sorting. Smaller expected loss is preferred
  # In the event of a tie, smaller sqErrorTau is prferred (tau closer to 0.5)
  # If still tied, larger values of lambda are prefered because they reduce the
  # number of predictors to create a more parsimonous model with fewer predictors
  dfData$sqErrorTau <- (dfData$tau - 0.5)^2
  gridMinimum <- sortDF(dfData, ~ExpectedLoss + sqErrorTau - lambda)[1,]

  # Add in the seed
  gridMinimum$seed <- seed


  # Return the optimal lambda, tau, and alpha for this particular seed
  return(gridMinimum[,c("seed", "alpha", "lambda", "tau", "ExpectedLoss")])

} # single_LRCbestsubsets

