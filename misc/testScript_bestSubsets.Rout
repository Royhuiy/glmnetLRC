
R version 3.1.1 (2014-07-10) -- "Sock it to Me"
Copyright (C) 2014 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> # For building during development only
> ## require(pnlStat)
> ## require(roxygen2)
> ## rma()
> ## unloadNamespace("lrc")
> ## roxygenize("~/rp/lrc", clean = TRUE)
> ## system("R CMD INSTALL  ~/rp/lrc")
> 
> 
> require(lrc)
Loading required package: lrc
Loading required package: glmnet
Loading required package: Matrix
Loaded glmnet 1.9-8

Loading required package: plyr
Loading required package: Smisc
Loading required package: parallel
Loading required package: bestglm
Loading required package: leaps


The package vignette can be found at /usr/local/R/R-3.1.1/library/lrc/doc/glmnetLRC.pdf


Please cite the following reference:
Amidan BG, Orton DJ, LaMarche BL, et al. 2014.
Signatures for Mass Spectrometry Data Quality.
Journal of Proteome Research. 13(4), 2215-2222.

> 
> ################################################################################
> # Test example (upon which to base the vignette) begins here
> ################################################################################
> 
> # Load the Mojave data
> data(Mojave)
> 
> # Here we select the predictor variables
> predictors <- Mojave[,-c(1,2,11)]
> 
> # And the response (presence/absence of cheat grass)
> cheat <- Mojave$cheatGrass
> 
> # Specify the loss matrix.
> # The "1" class is the target of interest (indicating the presence of cheatgrass).
> # The penalty for missing cheat grass is 2, while the penalty for predicting it
> # falsely is 1.
> lM <- lossMatrix(c("0","0","1","1"),
+                  c("0","1","0","1"), 
+                  c(0,   1,  2,  0))
> print(lM)
        Predicted.0 Predicted.1
Truth.0           0           1
Truth.1           2           0
> 
> # Train the elastic net classifier
> LRCbestsubsets_fit <- LRCbestsubsets(cheat, predictors, lM, cvReps = 100,
+                                      cvFolds = 5, cores = 7)
Morgan-Tatar search since family is non-gaussian.
> 
> 
> save(LRCbestsubsets_fit, file = "~/rp/lrc/data/LRCbestsubsets_fit.RData")
> 
> # LRCbestsubsets_fit <- loadObject("~/rp/lrc/data/LRCbestsubsets_fit.RData")
> 
> # Demonstrate the various methods (print, summary, plot, coef)
> print(LRCbestsubsets_fit)
The optimal threshold (tau) for the best subsets logistic regression fit: 
tau = 0.5 

Call:  glm(formula = y ~ ., family = family, data = Xi, weights = weights)

Coefficients:
(Intercept)      minNDVI  
  0.4970337    0.0004222  

Degrees of Freedom: 632 Total (i.e. Null);  631 Residual
Null Deviance:	    614.8 
Residual Deviance: 586.3 	AIC: 590.3
> 
> o <- print(LRCbestsubsets_fit)
The optimal threshold (tau) for the best subsets logistic regression fit: 
tau = 0.5 

Call:  glm(formula = y ~ ., family = family, data = Xi, weights = weights)

Coefficients:
(Intercept)      minNDVI  
  0.4970337    0.0004222  

Degrees of Freedom: 632 Total (i.e. Null);  631 Residual
Null Deviance:	    614.8 
Residual Deviance: 586.3 	AIC: 590.3
> o
    cvRepSeed  tau ExpectedLoss
1         732 0.45    0.1895735
2         411 0.35    0.1895735
3         876 0.50    0.1895735
4         477 0.50    0.1895735
5         494 0.50    0.1895735
6         642 0.50    0.1895735
7         605 0.50    0.1895735
8         652 0.45    0.1895735
9         530 0.55    0.1879937
10        596 0.50    0.1895735
11        741 0.50    0.1895735
12        991 0.50    0.1895735
13        928 0.50    0.1895735
14        840 0.45    0.1895735
15        239 0.40    0.1895735
16        861 0.50    0.1895735
17        976 0.50    0.1895735
18        724 0.40    0.1895735
19        336 0.50    0.1895735
20        100 0.35    0.1895735
21        783 0.35    0.1895735
22         14 0.50    0.1895735
23        903 0.50    0.1895735
24        712 0.50    0.1895735
25        500 0.50    0.1895735
26        435 0.50    0.1895735
27        111 0.45    0.1895735
28        661 0.50    0.1895735
29        333 0.50    0.1895735
30        598 0.50    0.1895735
31        438 0.50    0.1895735
32        777 0.50    0.1895735
33        391 0.40    0.1895735
34        779 0.50    0.1895735
35        519 0.50    0.1895735
36        353 0.50    0.1895735
37        655 0.50    0.1895735
38        381 0.50    0.1895735
39        294 0.50    0.1895735
40        651 0.45    0.1895735
41        553 0.50    0.1895735
42        338 0.50    0.1895735
43         36 0.50    0.1895735
44        285 0.40    0.1895735
45        316 0.50    0.1895735
46        869 0.60    0.1879937
47        181 0.40    0.1895735
48        877 0.35    0.1895735
49        615 0.50    0.1895735
50        575 0.35    0.1895735
51        329 0.50    0.1895735
52        208 0.50    0.1895735
53        912 0.50    0.1895735
54        390 0.50    0.1895735
55        985 0.50    0.1895735
56        384 0.50    0.1895735
57        482 0.50    0.1895735
58        453 0.40    0.1895735
59        789 0.50    0.1895735
60        640 0.50    0.1895735
61        880 0.50    0.1895735
62        723 0.50    0.1895735
63        187 0.50    0.1895735
64        590 0.50    0.1895735
65        326 0.50    0.1879937
66        729 0.50    0.1895735
67        413 0.50    0.1895735
68        122 0.50    0.1895735
69        339 0.50    0.1895735
70        380 0.40    0.1895735
71        944 0.50    0.1879937
72        177 0.50    0.1895735
73        647 0.50    0.1895735
74        715 0.45    0.1895735
75        599 0.60    0.1879937
76        934 0.50    0.1895735
77        383 0.50    0.1895735
78        190 0.50    0.1895735
79        693 0.35    0.1895735
80        960 0.50    0.1895735
81        573 0.50    0.1895735
82        895 0.50    0.1895735
83        246 0.50    0.1879937
84        794 0.50    0.1895735
85        202 0.50    0.1895735
86         77 0.50    0.1895735
87        526 0.50    0.1895735
88        770 0.50    0.1895735
89        687 0.50    0.1895735
90        130 0.50    0.1895735
91        101 0.50    0.1895735
92        432 0.50    0.1895735
93        389 0.50    0.1895735
94        601 0.50    0.1895735
95        943 0.50    0.1895735
96        662 0.40    0.1895735
97        629 0.50    0.1895735
98         62 0.50    0.1895735
99        410 0.50    0.1895735
100       864 0.35    0.1895735
> 
> summary(LRCbestsubsets_fit)
The optimal threshold (tau) for the best subsets logistic regression fit: 
tau = 0.5 

Call:
glm(formula = y ~ ., family = family, data = Xi, weights = weights)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4317   0.3837   0.5677   0.7433   0.9768  

Coefficients:
             Estimate Std. Error z value Pr(>|z|)    
(Intercept) 4.970e-01  2.027e-01   2.452   0.0142 *  
minNDVI     4.222e-04  8.594e-05   4.913 8.98e-07 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 614.77  on 632  degrees of freedom
Residual deviance: 586.26  on 631  degrees of freedom
AIC: 590.26

Number of Fisher Scoring iterations: 5

> 
> openDevice("~/tmp/testPackage.pdf")
> plot(LRCbestsubsets_fit)
> dev.off()
null device 
          1 
> 
> coef(LRCbestsubsets_fit)
 (Intercept)      minNDVI 
0.4970337013 0.0004221871 
> 
> # Calculate performance of the final model on all the training data
> out <- predict(LRCbestsubsets_fit, cbind(predictors, cheat),
+                truthCol = "cheat", keepCols = 1:3)
> 
> head(out)
  PredictClass cheat year Precip Burn
1            1     1 2012     25    1
2            1     1 2012     25    1
3            1     1 2012     25    1
4            1     1 2012     43    1
5            1     1 2012     43    1
6            1     1 2012     43    1
> summary(out)
                            1
sensitivity         1.0000000
specificity         0.0000000
false negative rate 0.0000000
false positive rate 1.0000000
accuracy            0.8104265
> 
> 
> proc.time()
   user  system elapsed 
  7.144   0.245 245.934 
