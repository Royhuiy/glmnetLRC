\documentclass{article}

\usepackage[left=1in, top=1in, right=1in, bottom=1in]{geometry}
\usepackage{graphicx, color}
\usepackage{url}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{framed}
\usepackage{here}
\usepackage{zi4}
\usepackage{color}
\usepackage{Sweave}
\renewcommand{\baselinestretch}{1.2}

% \VignetteIndexEntry{lrc Example}

\begin{document} 
\SweaveOpts{concordance=TRUE}

\title{{\tt lrc}: Logistic regression classification (LRC) with an arbitrary loss function\\}
\author{Landon Sego, Alexander Venzin}
\maketitle

\section{Introduction}

The {\tt lrc} package extends the {\tt glmnet} package by making it possible to train elastic net logistic 
regression classifiers (LRC's) using a customized, discrete loss function.  This allows users to assign unique loss values 
to false positive and false negative errors. This approach was originally implemented to automate the process 
of determining the curation quality of mass spectrometry samples. Some of the data documented in \cite{thepaper} 
will be used here to demonstrate how to train your own classifier. The elastic net parameter estimates are 
obtained by maximizing a penalized likelihood function. The penalty is essentially a weighted average of the 
ridge penalty ($\ell_2$ norm) and the lasso penalty ($\ell_1$ norm) of the regression parameters.  This approach 
balances feature selection and model simplicity. 

In the sections that follow, we show how to use the {\tt lrc} package to train LRC models, create diagnostic plots, extract
coefficients, predict the binary class of new observations, and summarize the performance of those predictions. The details of the algorithms used by the package are provided in the Appendix.

\section{Training}

Let's begin by loading the package and the training data:
<<data>>==
# Load the package
library(lrc)

# Load the VOrbitrap Shewanella QC data
data(traindata)

# A view of first five rows and first 12 columns
traindata[1:5, 1:12]

# Here we select the predictor variables
predictors <- as.matrix(traindata[,9:96])
@ 

\noindent We fit the LRC model by calling {\tt LRCglmnet()}, which 
requires a binary response variable, coded
as a {\tt factor}.  The order in which the response variable
is coded is important.  Specifically, the class we want to predict with
the greatest sensitivity should be encoded as the second level. To illustrate how this
is done, consider the Shewanella QC data, where the objective is to be
sensitive to predicting poor datasets.  Hence we code ``poor" last, as follows:
<<data1>>=
response <- factor(traindata$Curated_Quality,
                   levels = c("good", "poor"),
                   labels = c("good", "poor"))

levels(response)
@ 
\noindent Now we must define a discrete loss matrix. For the curation
of dataset quality, predicting ``good" when the dataset is ``poor" is considerably 
worse (Loss = 5) than predicting ``poor" when the dataset
is ``good" (Loss = 1).  Correct predictions receive a penalty of zero loss:

<<data2>>=
lM <- lossMatrix(c("good","good","poor","poor"),
                 c("good","poor","good","poor"),
                 c(     0,     1,     5,     0))

# Observe the structure of the loss matrix
lM
@ 

To train an elastic net model, the user needs to supply a handful of arguments to {\tt LRCglmnet()}. 
The mandatory arguments are the true class labels, {\tt truthLabels} (which, in this case, is, is the {\tt response} 
object we created above), the matrix of predictor variables, {\tt predictors}, 
and the loss matrix {\tt lossMat}. Noteworthy additional arguments include {\tt tauVec}, a vector of potential 
thresholds $\tau \in (0, 1)$ that are used to dichotomize the predicted probabilities from the logistic regression 
into two class labels; {\tt alphaVec}, a vector of potential values of the elastic net mixing parameter 
$\alpha \in [0, 1]$; {\tt cvFolds}, the number of cross validation folds; and {\tt masterSeed}, which controls 
the partitioning the data into the cross validation folds. Keep in mind that $\alpha$ governs the tradeoff between 
the two regularization penalties. When $\alpha = 0$, the objective is $\ell_2$ regularization (ridge regression) 
and when $\alpha = 1$, the objective is $\ell_1$ regularization (lasso regression).   

Be advised that heavier sampling of {\tt tauVec} or {\tt alphaVec} (i.e., sequences of greater length) leads to 
increased computation time, but more of the parameter space will be sampled, potentially leading to a better 
classifier. A step by step description of the algorithm that generates the classifier is provided in the appendix.  
We now train the elastic net logistic regression using the default settings for $\tau$ and the number of cross 
validation folds (which is 5) and restricting $\alpha = (0.5, 1)$:
<<train>>=
# Set the number of cores to be one less than the total available
ncores <- max(1, parallel::detectCores() - 1)

lrc_fit <- LRCglmnet(response, predictors, lM, nJobs = ncores,
                     alphaVec = c(1, 0.5), tauVec = c(0.3, 0.5, 0.7),
                     cvReps = 2 * ncores, estimateLoss = TRUE)

@ 

\noindent The call to {\tt LRCglmnet()} uses cross validation to solve for the optimal parameter settings 
$\left(\alpha, \lambda, \tau\right)$ that minimize the expected loss for the elastic net logistic regression 
classifier. Printing the resulting object shows the median value for the parameters over the cross validation 
replicates, as well as the average and standard deviation of the expected loss that is calculated for each
cross validation replicate.
 
<<checkdat>>==
print(lrc_fit)
@

\noindent We can also extract the coefficients of the logistic regression model that was created using the 
optimal values of $\alpha$ and $\lambda$ (which were shown by the call to the {\\tt print} method above):
<<getCoef>>==
coef(lrc_fit)
@

\section{Prediction}

Now that the classifier has been properly trained and the optimal parameters have been identified, we are 
interested in making predictions for new data observations. This requires the elastic net regression model 
(the output from {\tt LRCglmnet}) and the set of new observations to be predicted, {\tt newdata}.  If true labels 
are available in {\tt newdata}, the column containing these true class labels can be specified via the 
{\tt truthCol} argument. Additionally, one may wish to carry through a subset of the explanatory variables in 
{\tt newdata}.  These columns are indicated using {\tt keepCols}.   True labels are not required to make 
predictions---but they are required to compute performance metrics (sensitivity, specificity, etc.) for the 
elastic net logistic regression model. We begin by testing the classifer on the training data:
<<predictTrain>>=

predictTrain <- predict(lrc_fit, traindata, truthCol = "Curated_Quality", keepCols = 1:2)

# Look at beginning of the predicted data.  Note the extra columns that were kept.
head(predictTrain)

# Summarize the peformance of the new classifier in terms of a 
# variety of metrics:
summary(predictTrain)
@
\noindent Note how the sensitivity for detecting poor datsets is considerably better than the specificity. 
These results reflect a loss function that penalizes false negative errors associated with classifying a 
"poor" curation quality as "good" more than false positive errors (classifying a "good" curation quality 
as "poor"). 

\noindent Now let's bring in some new data and examine the performance of the classifier:
<<predict>>==
# load the data for testing
data(testdata)

# Create table observing the true number of good/poor items 
with(testdata, table(Curated_Quality))

# Predict new data
predictTest <- predict(lrc_fit,testdata,
                       truthCol = "Curated_Quality")

# Look at the first few rows
head(predictTest)

# Summarize the output of predicting the data we trained on 
summary(predictTest)
@ 

\section{Diagnostics}

\noindent Finally, we would like to get a sense of the distribution of the parameters that were chosen 
during the cross validation phase. The {\tt plot} method produces a 3 x 3 scatterplot matrix of the optimal 
triples $\left(\alpha, \lambda, \tau\right)$ associated with the selected regression model from each cross 
validation replicate. The univariate distribution of each parameter is plotted on the diagonal of the 
scatterplot matrix.  Ideally, the distributions of the parameters will be tight over the cross validation 
replicates, indicating that the choice of $\left(\alpha, \lambda, \tau\right)$ is stable regardless of
the particular random partition used for cross-validation.

\begin{figure}[H]
\begin{center}
<<plot, fig=TRUE, width=6, height=4>>=
plot(lrc_fit)
@
\caption{Scatterplot matrix of optimization parameters}
\end{center}
\end{figure}

\nocite{*}
\bibliography{lrc}
\bibliographystyle{plain}  

\newpage

\section*{Appendix}
We present in detail the algorithm used by the {\tt lrc} package to identify the optimal estimates of 
We provide below a high-level explanation of the parameter selection algorithm:


\end{document}
